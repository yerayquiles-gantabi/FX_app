{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error cuadrático medio (MSE): 5.568604070056622\n",
      "Error absoluto medio (MAE): 1.5335294818351282\n",
      "R² (coeficiente de determinación): 0.022383451461791992\n",
      "Varianza explicada: 0.028431014324293402\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "# Cargamos el dataset\n",
    "df = pd.read_csv('/home/ubuntu/STG-fractura_cadera/2025_02/models/PREPROCESADO_2/DATOS_PREPROCESADOS_2.csv')\n",
    "\n",
    "# Eliminar filas que contengan \"-999\" o -999 en cualquier columna\n",
    "df = df[~df.isin([\"-999\", -999]).any(axis=1)]\n",
    "\n",
    "# Eliminamos la columna identificadora\n",
    "df.drop(\"gidenpac\", axis=1, inplace=True)\n",
    "\n",
    "# Lista de variables categóricas que se desean tratar como tal\n",
    "cat_features = [\n",
    "    'gsitalta', 'itipsexo', 'itipingr', 'ireingre', 'iotrocen', 'gdiagalt',\n",
    "    'ds_izq_der', 'ds_turno', 'ds_dia_semana_llegada', 'ds_mes_llegada',\n",
    "    'ds_centro_afueras', 'ds_alergia_medicamentosa', 'ds_alergia_alimenticia',\n",
    "    'ds_otras_alergias', 'ds_ITU', 'ds_anemia',\n",
    "    'ds_vitamina_d', 'ds_insuficiencia_respiratoria', 'ds_insuficiencia_cardiaca',\n",
    "    'ds_deterioro_cognitivo', 'ds_insuficiencia_renal', 'ds_HTA', 'ds_diabetes'\n",
    "]\n",
    "\n",
    "# Convertir las variables listadas a tipo 'category'\n",
    "for col in cat_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Verificar los tipos de datos\n",
    "#print(\"Tipos de datos después de la conversión:\")\n",
    "#print(df.dtypes)\n",
    "\n",
    "# Definir las features y la variable objetivo.\n",
    "# Se eliminan las columnas que no se usarán como predictores.\n",
    "X = df.drop([\"gsitalta\", \"ds_vivo_alta\", \"ds_estancia\", \"ds_post_oper\", \"ds_pre_oper\"], axis=1)\n",
    "y = df[\"ds_pre_oper\"]\n",
    "\n",
    "# Identificar columnas numéricas y categóricas (se incluyen 'object' y 'category')\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "#print(\"\\nVariables categóricas que se utilizarán:\", list(cat_cols))\n",
    "\n",
    "# Crear un preprocesador que escala las variables numéricas y aplica OneHotEncoder a las categóricas\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])\n",
    "\n",
    "# Definir un pipeline que incluya el preprocesamiento y el modelo XGBoost\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", XGBRegressor(random_state=42, objective=\"reg:squarederror\"))\n",
    "])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir una búsqueda de hiperparámetros para XGBoost\n",
    "param_grid = {\n",
    "    \"regressor__n_estimators\": [100, 200, 300],\n",
    "    \"regressor__max_depth\": [3, 5, 7],\n",
    "    \"regressor__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"regressor__subsample\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#print(\"\\nMejores parámetros encontrados:\", grid_search.best_params_)\n",
    "\n",
    "# Realizar predicciones y evaluar el modelo\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nError cuadrático medio (MSE):\", mse)\n",
    "print(\"Error absoluto medio (MAE):\", mae)\n",
    "print(\"R² (coeficiente de determinación):\", r2)\n",
    "print(\"Varianza explicada:\", explained_var)\n",
    "\n",
    "# Verificar las columnas generadas tras el preprocesamiento\n",
    "preprocessor_final = grid_search.best_estimator_.named_steps[\"preprocessor\"]\n",
    "\n",
    "# Obtener los nombres de las columnas resultantes (requiere scikit-learn >= 1.0)\n",
    "try:\n",
    "    feature_names = preprocessor_final.get_feature_names_out()\n",
    "    #print(\"\\nCaracterísticas utilizadas por el modelo:\")\n",
    "    #print(feature_names)\n",
    "except AttributeError:\n",
    "    print(\"\\nLa versión de scikit-learn no soporta 'get_feature_names_out()'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LinearRegression ---\n",
      "MSE: 5.71515070667583\n",
      "MAE: 1.5881819751381216\n",
      "R²: -0.0033441525181225895\n",
      "Varianza explicada: 0.002719618156085124\n",
      "\n",
      "--- Ridge ---\n",
      "MSE: 5.692239554768118\n",
      "MAE: 1.582357876428939\n",
      "R²: 0.0006780984204579621\n",
      "Varianza explicada: 0.006699628023449811\n",
      "\n",
      "--- Lasso ---\n",
      "MSE: 5.732101590119526\n",
      "MAE: 1.5334399534748473\n",
      "R²: -0.0063200267611907\n",
      "Varianza explicada: -2.220446049250313e-16\n",
      "\n",
      "--- RandomForest ---\n",
      "MSE: 6.856547759975444\n",
      "MAE: 1.72355985267035\n",
      "R²: -0.2037262802880635\n",
      "Varianza explicada: -0.1731597210200717\n",
      "\n",
      "--- GradientBoosting ---\n",
      "MSE: 5.841003992918752\n",
      "MAE: 1.5869318131225363\n",
      "R²: -0.02543878576716807\n",
      "Varianza explicada: -0.01829449661906679\n",
      "\n",
      "--- XGBoost ---\n",
      "MSE: 7.286377687398138\n",
      "MAE: 1.8014932133216226\n",
      "R²: -0.2791866064071655\n",
      "Varianza explicada: -0.26064458827018666\n",
      "\n",
      "Comparación de modelos:\n",
      "              Model       MSE       MAE        R2  Explained Variance\n",
      "0  LinearRegression  5.715151  1.588182 -0.003344        2.719618e-03\n",
      "1             Ridge  5.692240  1.582358  0.000678        6.699628e-03\n",
      "2             Lasso  5.732102  1.533440 -0.006320       -2.220446e-16\n",
      "3      RandomForest  6.856548  1.723560 -0.203726       -1.731597e-01\n",
      "4  GradientBoosting  5.841004  1.586932 -0.025439       -1.829450e-02\n",
      "5           XGBoost  7.286378  1.801493 -0.279187       -2.606446e-01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv('/home/ubuntu/STG-fractura_cadera/2025_02/models/PREPROCESADO_2/DATOS_PREPROCESADOS_2.csv')\n",
    "\n",
    "# Eliminar filas con \"-999\" o -999\n",
    "df = df[~df.isin([\"-999\", -999]).any(axis=1)]\n",
    "df.drop(\"gidenpac\", axis=1, inplace=True)\n",
    "\n",
    "# Lista de variables categóricas a convertir\n",
    "cat_features = [\n",
    "    'gsitalta', 'itipsexo', 'itipingr', 'ireingre', 'iotrocen', 'gdiagalt',\n",
    "    'ds_izq_der', 'ds_turno', 'ds_dia_semana_llegada', 'ds_mes_llegada',\n",
    "    'ds_centro_afueras', 'ds_alergia_medicamentosa', \n",
    "    'ds_alergia_alimenticia', 'ds_otras_alergias', 'ds_ITU', 'ds_anemia',\n",
    "    'ds_vitamina_d', 'ds_insuficiencia_respiratoria', 'ds_insuficiencia_cardiaca',\n",
    "    'ds_deterioro_cognitivo', 'ds_insuficiencia_renal', 'ds_HTA', 'ds_diabetes'\n",
    "]\n",
    "\n",
    "# Convertir estas variables a tipo 'category'\n",
    "for col in cat_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Definir X e y. Se eliminan columnas que no se usarán como predictores\n",
    "X = df.drop([\"gsitalta\", \"ds_vivo_alta\", \"ds_estancia\", \"ds_post_oper\", \"ds_pre_oper\"], axis=1)\n",
    "y = df[\"ds_pre_oper\"]\n",
    "\n",
    "# Identificar columnas numéricas y categóricas\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "#print(\"Variables categóricas que se utilizarán:\", list(cat_cols))\n",
    "\n",
    "# Definir un preprocesador que escale las numéricas y codifique las categóricas\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir una lista de modelos a comparar\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(random_state=42),\n",
    "    \"Lasso\": Lasso(random_state=42),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42, objective=\"reg:squarederror\")\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Crear un pipeline para cada modelo\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", model)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    ev = explained_variance_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2\": r2,\n",
    "        \"Explained Variance\": ev\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"R²:\", r2)\n",
    "    print(\"Varianza explicada:\", ev)\n",
    "\n",
    "# Convertir los resultados a DataFrame para compararlos fácilmente\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparación de modelos:\")\n",
    "print(results_df)\n",
    "\n",
    "# Opcional: Si deseas ver las características finales utilizadas por el preprocesador en alguno de los pipelines,\n",
    "# puedes usar get_feature_names_out() (requiere scikit-learn >= 1.0)\n",
    "preprocessor_final = Pipeline(steps=[(\"preprocessor\", preprocessor)]).fit(X_train)\n",
    "try:\n",
    "    feature_names = preprocessor_final.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "    #print(\"\\nCaracterísticas utilizadas por el modelo:\")\n",
    "    #print(feature_names)\n",
    "except AttributeError:\n",
    "    print(\"\\nLa versión de scikit-learn no soporta 'get_feature_names_out()'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos de datos después de la conversión:\n",
      "itipsexo                         category\n",
      "itipingr                         category\n",
      "ireingre                         category\n",
      "gsitalta                         category\n",
      "iotrocen                         category\n",
      "gdiagalt                         category\n",
      "ds_izq_der                       category\n",
      "ds_turno                         category\n",
      "ds_edad                             int64\n",
      "ds_estancia                         int64\n",
      "ds_pre_oper                         int64\n",
      "ds_post_oper                        int64\n",
      "ds_vivo_alta                        int64\n",
      "ds_dia_semana_llegada            category\n",
      "ds_mes_llegada                   category\n",
      "ds_centro_afueras                category\n",
      "ds_alergia_medicamentosa         category\n",
      "ds_alergia_alimenticia           category\n",
      "ds_otras_alergias                category\n",
      "ds_ITU                           category\n",
      "ds_anemia                        category\n",
      "ds_vitamina_d                    category\n",
      "ds_insuficiencia_respiratoria    category\n",
      "ds_insuficiencia_cardiaca        category\n",
      "ds_deterioro_cognitivo           category\n",
      "ds_insuficiencia_renal           category\n",
      "ds_HTA                           category\n",
      "ds_diabetes                      category\n",
      "dtype: object\n",
      "\n",
      "Variables categóricas que se utilizarán: ['itipsexo', 'itipingr', 'ireingre', 'iotrocen', 'gdiagalt', 'ds_izq_der', 'ds_turno', 'ds_dia_semana_llegada', 'ds_mes_llegada', 'ds_centro_afueras', 'ds_alergia_medicamentosa', 'ds_alergia_alimenticia', 'ds_otras_alergias', 'ds_ITU', 'ds_anemia', 'ds_vitamina_d', 'ds_insuficiencia_respiratoria', 'ds_insuficiencia_cardiaca', 'ds_deterioro_cognitivo', 'ds_insuficiencia_renal', 'ds_HTA', 'ds_diabetes']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejores parámetros encontrados:\n",
      "{'feature_selection__n_features_to_select': 50, 'regressor__alpha': 0.01}\n",
      "\n",
      "Error cuadrático medio (MSE): 5.560806414975928\n",
      "Error absoluto medio (MAE): 1.5400963366501699\n",
      "R² (coeficiente de determinación): 0.023752323235483708\n",
      "Varianza explicada: 0.02935069662767975\n",
      "\n",
      "Características utilizadas tras el preprocesamiento:\n",
      "['num__ds_edad' 'cat__itipsexo_0' 'cat__itipsexo_1' 'cat__itipingr_0'\n",
      " 'cat__itipingr_1' 'cat__ireingre_0' 'cat__ireingre_1' 'cat__iotrocen_0'\n",
      " 'cat__iotrocen_1' 'cat__gdiagalt_S72.001A' 'cat__gdiagalt_S72.002A'\n",
      " 'cat__gdiagalt_S72.011A' 'cat__gdiagalt_S72.012A'\n",
      " 'cat__gdiagalt_S72.101A' 'cat__gdiagalt_S72.102A'\n",
      " 'cat__gdiagalt_S72.141A' 'cat__gdiagalt_S72.142A' 'cat__ds_izq_der_0'\n",
      " 'cat__ds_izq_der_1' 'cat__ds_turno_0' 'cat__ds_turno_1' 'cat__ds_turno_2'\n",
      " 'cat__ds_dia_semana_llegada_1' 'cat__ds_dia_semana_llegada_2'\n",
      " 'cat__ds_dia_semana_llegada_3' 'cat__ds_dia_semana_llegada_4'\n",
      " 'cat__ds_dia_semana_llegada_5' 'cat__ds_dia_semana_llegada_6'\n",
      " 'cat__ds_dia_semana_llegada_7' 'cat__ds_mes_llegada_1'\n",
      " 'cat__ds_mes_llegada_2' 'cat__ds_mes_llegada_3' 'cat__ds_mes_llegada_4'\n",
      " 'cat__ds_mes_llegada_5' 'cat__ds_mes_llegada_6' 'cat__ds_mes_llegada_7'\n",
      " 'cat__ds_mes_llegada_8' 'cat__ds_mes_llegada_9' 'cat__ds_mes_llegada_10'\n",
      " 'cat__ds_mes_llegada_11' 'cat__ds_mes_llegada_12'\n",
      " 'cat__ds_centro_afueras_0' 'cat__ds_centro_afueras_1'\n",
      " 'cat__ds_alergia_medicamentosa_0' 'cat__ds_alergia_medicamentosa_1'\n",
      " 'cat__ds_alergia_alimenticia_0' 'cat__ds_alergia_alimenticia_1'\n",
      " 'cat__ds_otras_alergias_0' 'cat__ds_otras_alergias_1' 'cat__ds_ITU_0'\n",
      " 'cat__ds_ITU_1' 'cat__ds_anemia_0' 'cat__ds_anemia_1'\n",
      " 'cat__ds_vitamina_d_0' 'cat__ds_vitamina_d_1'\n",
      " 'cat__ds_insuficiencia_respiratoria_0'\n",
      " 'cat__ds_insuficiencia_respiratoria_1' 'cat__ds_insuficiencia_cardiaca_0'\n",
      " 'cat__ds_insuficiencia_cardiaca_1' 'cat__ds_deterioro_cognitivo_0'\n",
      " 'cat__ds_deterioro_cognitivo_1' 'cat__ds_insuficiencia_renal_0'\n",
      " 'cat__ds_insuficiencia_renal_1' 'cat__ds_HTA_0' 'cat__ds_HTA_1'\n",
      " 'cat__ds_diabetes_0' 'cat__ds_diabetes_1']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "# Cargamos el dataset\n",
    "df = pd.read_csv('/home/ubuntu/STG-fractura_cadera/models_v2/PREPROCESADO_2/DATOS_PREPROCESADOS_2.csv')\n",
    "\n",
    "# Eliminar filas que contengan \"-999\" o -999 en cualquier columna\n",
    "df = df[~df.isin([\"-999\", -999]).any(axis=1)]\n",
    "\n",
    "# Eliminamos la columna identificadora\n",
    "df.drop(\"gidenpac\", axis=1, inplace=True)\n",
    "\n",
    "# Lista de variables categóricas a tratar como tales\n",
    "cat_features = [\n",
    "    'gsitalta', 'itipsexo', 'itipingr', 'ireingre', 'iotrocen', 'gdiagalt',\n",
    "    'ds_izq_der', 'ds_turno', 'ds_dia_semana_llegada', 'ds_mes_llegada',\n",
    "    'ds_centro_afueras', 'ds_alergia_medicamentosa', 'movilidad', 'riesgo_caida',\n",
    "    'ds_alergia_alimenticia', 'ds_otras_alergias', 'ds_ITU', 'ds_anemia',\n",
    "    'ds_vitamina_d', 'ds_insuficiencia_respiratoria', 'ds_insuficiencia_cardiaca',\n",
    "    'ds_deterioro_cognitivo', 'ds_insuficiencia_renal', 'ds_HTA', 'ds_diabetes'\n",
    "]\n",
    "\n",
    "# Convertir las variables de la lista a tipo 'category'\n",
    "for col in cat_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Verificar los tipos de datos\n",
    "print(\"Tipos de datos después de la conversión:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Definir X e y.\n",
    "# Se eliminan las columnas que no se utilizarán como predictores\n",
    "X = df.drop([\"gsitalta\", \"ds_vivo_alta\", \"ds_estancia\", \"ds_post_oper\", \"ds_pre_oper\"], axis=1)\n",
    "y = df[\"ds_pre_oper\"]\n",
    "\n",
    "# Identificar columnas numéricas y categóricas\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "print(\"\\nVariables categóricas que se utilizarán:\", list(cat_cols))\n",
    "\n",
    "# Preprocesador: uso de RobustScaler para las variables numéricas y OneHotEncoder para las categóricas\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", RobustScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])\n",
    "\n",
    "# Usaremos Lasso con validación cruzada para la selección de características.\n",
    "# Esta será la base para el RFE y también el modelo final.\n",
    "# Se puede optimizar el parámetro alpha.\n",
    "base_estimator = LassoCV(cv=5, random_state=42)\n",
    "\n",
    "# Definir RFE. El número de features a seleccionar se optimizará.\n",
    "rfe = RFE(estimator=base_estimator, n_features_to_select=30)\n",
    "\n",
    "# Modelo final: Usamos Lasso (puede ser el mismo que en la selección)\n",
    "regressor = Lasso(random_state=42)\n",
    "\n",
    "# Pipeline: preprocesador -> selección de características (RFE) -> regresor\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", rfe),\n",
    "    (\"regressor\", regressor)\n",
    "])\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros para optimizar:\n",
    "# - El número de features a seleccionar en RFE\n",
    "# - El parámetro de regularización de Lasso final (alpha)\n",
    "param_grid = {\n",
    "    \"feature_selection__n_features_to_select\": [20, 30, 40, 50],\n",
    "    \"regressor__alpha\": [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nMejores parámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predicciones y evaluación\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nError cuadrático medio (MSE):\", mse)\n",
    "print(\"Error absoluto medio (MAE):\", mae)\n",
    "print(\"R² (coeficiente de determinación):\", r2)\n",
    "print(\"Varianza explicada:\", explained_var)\n",
    "\n",
    "# Verificar las características utilizadas por el modelo\n",
    "preprocessor_final = grid_search.best_estimator_.named_steps[\"preprocessor\"]\n",
    "try:\n",
    "    feature_names = preprocessor_final.get_feature_names_out()\n",
    "    print(\"\\nCaracterísticas utilizadas tras el preprocesamiento:\")\n",
    "    print(feature_names)\n",
    "except AttributeError:\n",
    "    print(\"\\nLa versión de scikit-learn no soporta 'get_feature_names_out()'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
