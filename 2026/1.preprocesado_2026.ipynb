{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib  # Librería estándar para guardar modelos y scalers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Obtener los medicamentos mas relevantes\n",
    "- Intentar obtener lugar_residencia, lugar_procedencia, destino_alta y vive_solo --> No tenemos datos\n",
    "- Ver como hago la columna gdiagalt --> De momento lo dejaria con los códigos, no hay una clasificación clara.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1892/1391586130.py:5: DtypeWarning: Columns (57) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_prescripcion_farmacos = pd.read_csv('data/data_2018_01_01-2026_01_13/PrescripcionFarmacos_combinado.csv')\n",
      "/tmp/ipykernel_1892/1391586130.py:7: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_constantes = pd.read_csv('data/data_2018_01_01-2026_01_13/ConstantesVitales_combinado.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/data_2018_01_01-2026_01_13/DatosIdentificativosAdministrativos_combinado.csv')\n",
    "df_glucemias = pd.read_csv('data/data_2018_01_01-2026_01_13/GlucemiasGlucosurias_combinado.csv')\n",
    "df_antecedentes = pd.read_csv('data/data_2018_01_01-2026_01_13/ValoracionesAntecedentesPersonales_combinado.csv')\n",
    "df_valoracion_enfermeria = pd.read_csv('data/data_2018_01_01-2026_01_13/ValoracionesEnfermeria_combinado.csv')\n",
    "df_prescripcion_farmacos = pd.read_csv('data/data_2018_01_01-2026_01_13/PrescripcionFarmacos_combinado.csv')\n",
    "df_valoracion_medica = pd.read_csv('data/data_2018_01_01-2026_01_13/ValoracionesMedicas_combinado.csv')\n",
    "df_constantes = pd.read_csv('data/data_2018_01_01-2026_01_13/ConstantesVitales_combinado.csv')\n",
    "\n",
    "#df_accidentes = pd.read_csv('../datos/AccidentesPlanta.csv')\n",
    "#df_dietas = pd.read_csv('../datos/DietasViasAlimentacion.csv')\n",
    "#df_infecciones = pd.read_csv('../datos/InfeccionesNosocomiales.csv')\n",
    "#df_ulceras = pd.read_csv('data_2025_06_01-2026_01_11/UlcerasPresion_combinado.csv')\n",
    "#df_prestaciones =  pd.read_csv('data_2025_06_01-2026_01_11/Prestaciones_combinado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos identificaticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas que no tienen información relevante\n",
    "df = df.drop(columns=['gprovinc', 'dnomprov', 'gpoblaci','dpoblaci','odiagini', 'odiagalt','dnomprov','ganoadme','gnumadme','gmotingr','dmotingr','gdiaging','gdiagini', 'gserulti', 'dnomserv','dsitalta','dmotalta','itraslad','odiaging','gdiasec1', 'odiasec1', 'gdiasec2', 'odiasec2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1892/3855050951.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['itipsexo'] = df['itipsexo'].replace({'H': 0, 'M': 1})\n",
      "/tmp/ipykernel_1892/3855050951.py:100: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['ireingre'] = df['ireingre'].replace({'N': 0, 'S': 1})\n",
      "/tmp/ipykernel_1892/3855050951.py:116: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['iotrocen'] = df['iotrocen'].replace({'N': 0, 'S': 1})\n"
     ]
    }
   ],
   "source": [
    "# Reemplazar 'H' por 0 y 'M' por 1 en la columna 'itipsexo'\n",
    "df['itipsexo'] = df['itipsexo'].replace({'H': 0, 'M': 1})\n",
    "\n",
    "\n",
    "# Columna ds_izq_der para saber que lado de la cadera es \n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Listas de códigos para \"DERECHO\" y \"IZQUIERDO\"\n",
    "derecho_codes = [\"M66.151\",\"M80.051A\",\"M97.01XA\",\"M97.01XD\", \"M97.01XS\",\"S72.001A\",\"S72.011A\",\"S72.091A\",\"S72.101A\",\"S72.141A\",\"S73.031A\",\"S79.811A\", \"T84.010\",\"T84.010A\",\"T84.010D\",\"T84.010S\",\"T84.040\",\"T84.040A\",\"T84.040D\",\"T84.040S\", \"T84.090A\"]\n",
    "izquierdo_codes = [\"M66.152\",\"M97.02XA\",\"M97.02XD\",\"M97.02XS\", \"S70.02XA\",\"S72.002A\",\"S72.012A\",\"S72.102A\",\"S72.112A\",\"S72.142A\",\"S72.142D\",\"S72.22XA\",\"T84.011\", \"T84.011A\",\"T84.011D\",\"T84.011S\",\"T84.021A\",\"T84.031A\",\"T84.041\", \"T84.041A\",\"T84.041D\",\"T84.041S\",]\n",
    "no_especificado = [\"M66.15\",\"M66.159\",\"M84.359\",\"M84.359A\",\"M84.359D\",\"M84.359G\",\"M84.359K\",\"M84.359P\",\"M84.359S\",\"M84.459\",\"M84.459A\",\"M84.459D\",\"M84.459G\",\"M84.459K\",\"M84.459P\",\"M84.459S\",\"M84.559\",\"M84.559A\",\"M84.559D\",\"M84.559G\",\"M84.559K\",\n",
    "                    \"M84.559P\",\"M84.559S\",\"M84.659\",\"M84.659A\",\"M84.659D\",\"M84.659G\",\"M84.659K\",\"M84.659P\",\"M84.659S\",\"T84.84XA\",\"Z51.89\"]\n",
    "# Categorizar la columna\n",
    "def categorize_side(value):\n",
    "    value = str(value)  # Convertir el valor a string por seguridad\n",
    "    if any(code in value for code in no_especificado):\n",
    "        return 0\n",
    "    elif any(code in value for code in izquierdo_codes):\n",
    "        return 1\n",
    "    elif any(code in value for code in derecho_codes):\n",
    "        return 2\n",
    "    else:\n",
    "        return None  # Marcar para eliminar\n",
    "\n",
    "# Crear la nueva columna 'ds_izq_der' basada en la función de categorización\n",
    "df['ds_izq_der'] = df['gdiagalt'].apply(categorize_side)\n",
    "df['ds_izq_der'] = pd.to_numeric(df['ds_izq_der'], errors='coerce').fillna(-999).astype(int)\n",
    "\n",
    "\n",
    "# Eliminar las filas donde 'ds_izq_der' es None \n",
    "df = df[(df['ds_izq_der'] != -999)]\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# Columna turnos de trabajo\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Convertir la columna 'fllegada' a formato de fecha y hora\n",
    "df['fllegada'] = pd.to_datetime(df['fllegada'])\n",
    "\n",
    "# Función para asignar el turno en función de la hora\n",
    "def asignar_turno(hora):\n",
    "    if 8 <= hora < 15:      #Mañana\n",
    "        return 0\n",
    "    elif 15 <= hora < 22:   # Tarde\n",
    "        return 1\n",
    "    else:                   # Noche\n",
    "        return 2\n",
    "\n",
    "# Crear una nueva columna 'ds_turno' basada en la hora de 'fllegada'\n",
    "df['ds_turno'] = df['fllegada'].dt.hour.apply(asignar_turno)\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# Columna edad\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Convertir la columna 'fnacipac' a formato de fecha\n",
    "df['fnacipac'] = pd.to_datetime(df['fnacipac'])\n",
    "\n",
    "# Función para calcular la edad\n",
    "def calcular_edad(fecha_nacimiento):\n",
    "    hoy = datetime.now()\n",
    "    edad = hoy.year - fecha_nacimiento.year - ((hoy.month, hoy.day) < (fecha_nacimiento.month, fecha_nacimiento.day))\n",
    "    return edad\n",
    "\n",
    "# Crear la nueva columna 'ds_edad' calculando la edad para cada fila\n",
    "df['ds_edad'] = df['fnacipac'].apply(calcular_edad)\n",
    "df = df[df['ds_edad'] >= 50] # Filtrar pacientes mayores de 50 años, menores hay muy pocos y estropea la muestra\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# Columnas de días estancia, preopertorio y postoperatorio\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Convertir las columnas de fecha a formato datetime, sin considerar las horas\n",
    "df['fllegada'] = pd.to_datetime(df['fllegada']).dt.normalize()\n",
    "df['faltplan'] = pd.to_datetime(df['faltplan']).dt.normalize()\n",
    "df['finterve'] = pd.to_datetime(df['finterve']).dt.normalize()\n",
    "\n",
    "# Calcular los días de estancia total (faltplan - fllegada)\n",
    "df['ds_estancia'] = (df['faltplan'] - df['fllegada']).dt.days\n",
    "df = df[df['ds_estancia'] <= 50] # Eliminar estancias mayores a 50 días, que son outliers\n",
    "\n",
    "# Calcular los días de preoperatorio incluyendo el día de intervención\n",
    "df['ds_pre_oper'] = (df['finterve'] - df['fllegada']).dt.days\n",
    "df['ds_pre_oper'] = pd.to_numeric(df['ds_pre_oper'], errors='coerce').fillna(-999).astype(int)\n",
    "#df = df[df['ds_pre_oper'] <= 20] # Eliminar estancias mayores a 20 días, que son outliers\n",
    "\n",
    "\n",
    "# Calcular los días de postoperatorio incluyendo el día de intervención\n",
    "df['ds_post_oper'] = (df['faltplan'] - df['finterve']).dt.days\n",
    "df['ds_post_oper'] = pd.to_numeric(df['ds_post_oper'], errors='coerce').fillna(-999).astype(int)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Eliminar filas donde 'gsitalta' es NaN (vacia) antes de calcular si vive o no\n",
    "df.dropna(subset=['gsitalta'], inplace=True)\n",
    "df['gsitalta'] = df['gsitalta'].astype(int)\n",
    "df = df[df['gsitalta'] != 0]\n",
    "\n",
    "\n",
    "# Crear la columna 'ds_vivo_alta' basada en la condición de 'gsitalta'\n",
    "df['ds_vivo_alta'] = df['gsitalta'].apply(lambda x: 0 if x == 7 else 1)\n",
    "\n",
    "# Reemplazar 'N' por 0 y 'S' por 1 en la columna 'ireingre'\n",
    "df['ireingre'] = df['ireingre'].replace({'N': 0, 'S': 1})\n",
    "\n",
    "# Crear la columna 'ds_dia_semana_llegada'(1 para lunes, 7 para domingo)\n",
    "df['ds_dia_semana_llegada'] = df['fllegada'].dt.dayofweek + 1\n",
    "\n",
    "# Crear la columna 'ds_mes_llegada'  \n",
    "df['ds_mes_llegada'] = df['fllegada'].dt.month\n",
    "\n",
    "# Obtenemos si viven en el centro o en las afueras\n",
    "codigos_centro = [24001, 24002, 24003, 24004, 24005, 24006, 24007, 24008, 24009, 24010, 24012, 24070, 24071, 24080, ]  #Códigos del centro\n",
    "df['ds_centro_afueras'] = df['gcodipos'].apply(lambda x: 1 if x in codigos_centro else 0)\n",
    "\n",
    "# Reemplazar 'I' por 0 y 'U' por 1 en la columna 'itipingr'\n",
    "df['itipingr'] = df['itipingr'].replace({'I': 0, 'U': 1})\n",
    "\n",
    "# Reemplazar 'N' por 0 y 'S' por 1 en la columna 'iotrocen'\n",
    "df['iotrocen'] = df['iotrocen'].replace({'N': 0, 'S': 1})\n",
    "\n",
    "# Eliminar pacientes que no tienen nada que ver con Fractura de cadera\n",
    "# Lista de valores que quieres eliminar\n",
    "valores_a_eliminar = ['FDFD192912290', 'FRGR193404470', 'GNMR193911510']\n",
    "\n",
    "# Eliminar filas donde gidenpac tenga valores en la lista\n",
    "df = df[~df['gidenpac'].isin(valores_a_eliminar)]\n",
    "\n",
    "# Eliminar columnas que ya no necesitamos\n",
    "#df = df.drop(columns=['faltplan'])\n",
    "#df = df.drop(columns=['fnacipac'])\n",
    "#df = df.drop(columns=['fllegada'])\n",
    "#df = df.drop(columns=['finterve'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accidentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Filtrar df_accidentes para seleccionar solo las columnas relevantes\\ndf_accidentes = df_accidentes[['gidenpac', 'ifamilia', 'gmovilid']]\\n\\n# Realizar el merge con df, añadiendo las columnas de 'ifamilia' donde coincida 'gidenpac'\\ndf = pd.merge(df, df_accidentes, on='gidenpac', how='left')\\n\\n# Reemplazar 'N' por 0 y 'S' por 1 en la columna 'ifamilia'\\ndf['ifamilia'] = df['ifamilia'].replace({'N': 0, 'S': 1})\\n\\n# Cambiar el nombre de la columna 'ifamilia' a 'vive_solo'\\ndf = df.rename(columns={'ifamilia': 'vive_solo'})\\n\\n# Renombrar 'gmovilid' como 'movilidad_accidentes'\\ndf = df.rename(columns={'gmovilid': 'movilidad_accidentes'})\\n\\n# Eliminar filas duplicadas basándose en la columna 'gidenpac'\\ndf = df.drop_duplicates(subset='gidenpac')\\n# Pasar a entero y añadimos -999 en los vacios\\n#df['vive_solo'] = df['vive_solo'].fillna(-999).astype(int)\\n#df['movilidad_accidentes'] = df['movilidad_accidentes'].fillna(-999).astype(int)\\n\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No tiene info que podamos usar\n",
    "\"\"\"\n",
    "# Filtrar df_accidentes para seleccionar solo las columnas relevantes\n",
    "df_accidentes = df_accidentes[['gidenpac', 'ifamilia', 'gmovilid']]\n",
    "\n",
    "# Realizar el merge con df, añadiendo las columnas de 'ifamilia' donde coincida 'gidenpac'\n",
    "df = pd.merge(df, df_accidentes, on='gidenpac', how='left')\n",
    "\n",
    "# Reemplazar 'N' por 0 y 'S' por 1 en la columna 'ifamilia'\n",
    "df['ifamilia'] = df['ifamilia'].replace({'N': 0, 'S': 1})\n",
    "\n",
    "# Cambiar el nombre de la columna 'ifamilia' a 'vive_solo'\n",
    "df = df.rename(columns={'ifamilia': 'vive_solo'})\n",
    "\n",
    "# Renombrar 'gmovilid' como 'movilidad_accidentes'\n",
    "df = df.rename(columns={'gmovilid': 'movilidad_accidentes'})\n",
    "\n",
    "# Eliminar filas duplicadas basándose en la columna 'gidenpac'\n",
    "df = df.drop_duplicates(subset='gidenpac')\n",
    "# Pasar a entero y añadimos -999 en los vacios\n",
    "#df['vive_solo'] = df['vive_solo'].fillna(-999).astype(int)\n",
    "#df['movilidad_accidentes'] = df['movilidad_accidentes'].fillna(-999).astype(int)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes vitales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1892/910770467.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  vitals_cercanas = df.groupby('gidenpac').apply(lambda g: get_closest_vitals(g, df_constantes))\n"
     ]
    }
   ],
   "source": [
    "# Convertir fechas una sola vez\n",
    "df['fllegada'] = pd.to_datetime(df['fllegada'], errors='coerce')\n",
    "df_constantes['fapuntes'] = pd.to_datetime(df_constantes['fapuntes'], errors='coerce')\n",
    "\n",
    "# Función optimizada\n",
    "def get_closest_vitals(group, df_const):\n",
    "    \"\"\"Para cada paciente, encuentra el registro más cercano\"\"\"\n",
    "    gidenpac = group.name\n",
    "    fllegada = group['fllegada'].iloc[0]\n",
    "    \n",
    "    # Filtrar solo registros de este paciente\n",
    "    registros = df_const[df_const['gidenpac'] == gidenpac].copy()\n",
    "    if registros.empty:\n",
    "        return pd.Series({'ntensmin': -999, 'ntensmax': -999, 'ntempera': -999.0, 'nsatuoxi': -999})\n",
    "    \n",
    "    # Calcular distancia\n",
    "    registros['diff'] = (registros['fapuntes'] - fllegada).abs()\n",
    "    mas_cercano = registros.loc[registros['diff'].idxmin()]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'ntensmin': mas_cercano['ntensmin'],\n",
    "        'ntensmax': mas_cercano['ntensmax'],\n",
    "        'ntempera': mas_cercano['ntempera'],\n",
    "        'nsatuoxi': mas_cercano['nsatuoxi']\n",
    "    })\n",
    "\n",
    "# Aplicar por paciente (mucho más rápido)\n",
    "vitals_cercanas = df.groupby('gidenpac').apply(lambda g: get_closest_vitals(g, df_constantes))\n",
    "df = df.join(vitals_cercanas, on='gidenpac')\n",
    "\n",
    "# Conversiones\n",
    "df['ntensmin'] = pd.to_numeric(df['ntensmin'], errors='coerce').fillna(-999).astype(int)\n",
    "df['ntensmax'] = pd.to_numeric(df['ntensmax'], errors='coerce').fillna(-999).astype(int)\n",
    "df['ntempera'] = pd.to_numeric(df['ntempera'], errors='coerce').fillna(-999).astype(float)\n",
    "df['nsatuoxi'] = pd.to_numeric(df['nsatuoxi'], errors='coerce').fillna(-999).astype(int)\n",
    "\n",
    "# Filtros\n",
    "df = df[(df['ntensmin'] >= 30) & (df['ntensmin'] <= 105)]\n",
    "df = df[(df['ntensmax'] >= 40) & (df['ntensmax'] <= 200)]\n",
    "df = df[df['nsatuoxi'] >= 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antecedentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1892/2776989086.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_ant.replace({'N': 0, 'S': 1, 'A': 1}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar solo las columnas necesarias de 'df_antecedentes' y crear una copia\n",
    "df_ant = df_antecedentes[['gidenpac', 'dconclin', 'vbivalor']].copy()\n",
    "\n",
    "# Crear columnas en 'df_ant' basadas en el contenido de 'dconclin'\n",
    "df_ant['ds_HTA'] = df_ant[\"dconclin\"].str.contains(\"HTA\", na=False).astype(int)\n",
    "df_ant['ds_alergia_medicamentosa'] = df_ant[\"dconclin\"].str.contains(\"Alergia medicamentosa\").astype(int)\n",
    "df_ant['ds_alergia_alimenticia'] = df_ant[\"dconclin\"].str.contains(\"Alergia alimenticia\").astype(int)\n",
    "df_ant['ds_diabetes'] = df_ant[\"dconclin\"].str.contains(\"Diabetes Mellitus\").astype(int)\n",
    "df_ant['ds_otras_alergias'] = df_ant[\"dconclin\"].str.contains(\"Otras alergias\").astype(int)\n",
    "\n",
    "# Actualizar valores en columnas de antecedentes usando 'vbivalor' si no es nulo\n",
    "columns_to_update = ['ds_HTA', 'ds_alergia_medicamentosa', 'ds_alergia_alimenticia', 'ds_otras_alergias']\n",
    "for col in columns_to_update:\n",
    "    df_ant[col] = df_ant.apply(lambda x: x['vbivalor'] if pd.notnull(x['vbivalor']) and x[col] == 1 else x[col], axis=1)\n",
    "\n",
    "# Reemplazar valores específicos ('N' -> 0, 'S' -> 1, 'A' -> 1) en todo el DataFrame\n",
    "df_ant.replace({'N': 0, 'S': 1, 'A': 1}, inplace=True)\n",
    "\n",
    "# Eliminar las columnas 'dconclin' y 'vbivalor' si ya no son necesarias\n",
    "df_ant.drop(columns=['dconclin', 'vbivalor'], inplace=True)\n",
    "\n",
    "# Eliminar duplicados en el DataFrame (si aún no se ha hecho)\n",
    "df_ant = df_ant.drop_duplicates()\n",
    "\n",
    "# Agrupar por 'gidenpac' y usar 'max' para que cualquier 1 en una columna mantenga el valor como 1\n",
    "df_ant = df_ant.groupby('gidenpac', as_index=False).max()\n",
    "\n",
    "# Realizar un merge entre df y df_ant sin sufijos\n",
    "df = pd.merge(df, df_ant, on='gidenpac', how='left')\n",
    "\n",
    "# Asignar valores de df_ant solo si son 1\n",
    "for col in columns_to_update:\n",
    "    df[col] = df.apply(lambda x: x[col] if pd.notnull(x[col]) and x[col] == 1 else np.nan if pd.isna(x[col]) else x[col], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valoracion Enfermeria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo las columnas necesarias de 'df_valoracion_enfermeria'\n",
    "df_valoracion_enfermeria = df_valoracion_enfermeria[['gidenpac', 'dconclin', 'ncodtabu', 'nsecuval', 'nvalncon', 'ovallcon', 'vbivalor']].copy()\n",
    "\n",
    "# Asegurarse de que 'ncodtabu', 'nvalncon', 'ovallcon', y 'vbivalor' sean numéricos\n",
    "df_valoracion_enfermeria[['ncodtabu', 'nvalncon', 'ovallcon', 'vbivalor']] = df_valoracion_enfermeria[['ncodtabu', 'nvalncon', 'ovallcon', 'vbivalor']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Definir los valores de 'dconclin' que se convertirán en nuevas columnas\n",
    "patterns = {\n",
    "    '_movilidad': \"_Movilidad\",\n",
    "    'movilidad': \"Movilidad\",\n",
    "    'lugar_residencia': \"G Lugar de Residencia\",\n",
    "    'lugar_procedencia': \"G Procedencia\",\n",
    "    'destino_alta': \"G Destino Alta\",\n",
    "    'Barthel': \"Resultado Indice de Barthel\",\n",
    "    'barthel_alta': \"Resultado Indice de Barthel al Alta\",\n",
    "    'braden': \"Resultado Escala de Braden\",\n",
    "    'riesgo_caida': \"Resultado Escala Riesgo Caidas\"\n",
    "}\n",
    "\n",
    "# Crear una expresión regular unida para filtrar las filas deseadas\n",
    "regex_pattern = \"|\".join(patterns.values())\n",
    "\n",
    "# Filtrar el DataFrame para conservar solo las filas que coinciden con el patrón\n",
    "df_valoracion_enfermeria = df_valoracion_enfermeria[df_valoracion_enfermeria['dconclin'].str.contains(regex_pattern, na=False)]\n",
    "\n",
    "# Iterar sobre el diccionario para crear las columnas basadas en el valor máximo entre las columnas especificadas\n",
    "for new_col, pattern in patterns.items():\n",
    "    # Crear la nueva columna con el valor máximo de las columnas seleccionadas donde 'dconclin' coincide con el patrón\n",
    "    df_valoracion_enfermeria[new_col] = np.where(\n",
    "        df_valoracion_enfermeria['dconclin'].str.contains(pattern, na=False),\n",
    "        df_valoracion_enfermeria[['ncodtabu', 'nvalncon', 'ovallcon', 'vbivalor']].max(axis=1),\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "# Agrupar por 'gidenpac' y mantener el valor máximo en cada columna para eliminar duplicados\n",
    "df_valoracion_enfermeria = df_valoracion_enfermeria.groupby('gidenpac', as_index=False).max()\n",
    "\n",
    "# Eliminar las columnas no deseadas del DataFrame\n",
    "df_valoracion_enfermeria = df_valoracion_enfermeria.drop(columns=['dconclin', 'ncodtabu', 'nsecuval', 'nvalncon', 'ovallcon', 'vbivalor', '_movilidad','barthel_alta'])\n",
    "\n",
    "# Combinar df y df_valoracion_enfermeria por la columna 'gidenpac'\n",
    "df= pd.merge(df, df_valoracion_enfermeria, on='gidenpac', how='left')\n",
    "\n",
    "# Convertimos a enteros y los Nan los pasamos a -999\n",
    "df['movilidad'] = df['movilidad'].fillna(-999).astype(int)\n",
    "df = df[df['movilidad'] != -999] # Mantenemos solo los registros donde movilidad NO sea -999\n",
    "df = df[df['movilidad'] <= 4] # Eliminamos registros con movilidad mayor a 4, que no tienen sentido\n",
    "\n",
    "df['lugar_residencia'] = df['lugar_residencia'].fillna(-999).astype(int)\n",
    "df['lugar_procedencia'] = df['lugar_procedencia'].fillna(-999).astype(int)\n",
    "df['destino_alta'] = df['destino_alta'].fillna(-999).astype(int)\n",
    "df['Barthel'] = df['Barthel'].fillna(-999).astype(int)\n",
    "df = df[df['Barthel'] != -999] # Mantenemos solo los registros que no sean -999\n",
    "df['braden'] = df['braden'].fillna(-999).astype(int)\n",
    "df = df[df['braden'] != -999] # Mantenemos solo los registros que no sean -999\n",
    "df['riesgo_caida'] = df['riesgo_caida'].fillna(-999).astype(int)\n",
    "df = df[df['riesgo_caida'] != -999] # Mantenemos solo los registros donde riesgo_caida NO sea -999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glucemias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pocos datos\n",
    "\n",
    "# Crear un conjunto con los valores de gidenpac en df_glucemias\n",
    "glucemias_pacientes = set(df_glucemias['gidenpac'].dropna())\n",
    "\n",
    "# Crear la columna 'ds_diabetes' solo en las casillas vacías. Añadimos info a la casilla ds_diabetes\n",
    "df['ds_diabetes'] = df.apply(lambda row: 1 if pd.isna(row['ds_diabetes']) and row['gidenpac'] in glucemias_pacientes \n",
    "                             else (0 if pd.isna(row['ds_diabetes']) else row['ds_diabetes']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valoraciones Medicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ovallcon\n",
      "LOS PREVIOS                                           2087\n",
      "HTA                                                   1165\n",
      "FRACTURA SUBCAPITAL DE CADERA DERECHA                  879\n",
      "FRACTURA PERTROCANTEREA DE CADERA DERECHA              872\n",
      "DISLIPEMIA                                             480\n",
      "                                                      ... \n",
      "FRACTURA SUBCAPITAL DE FÉMUR DERECHO FEMUR DERECHO       1\n",
      "ÚLCERA POR PRESIÓN ESTADIO IV                            1\n",
      "FRACTURA DE LA CADERA IZQ                                1\n",
      "S.IQUAW+PIE EQUINO IZDO                                  1\n",
      "DIABETES MELLITUS TIPO2                                  1\n",
      "Name: count, Length: 8131, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2. Definir los códigos\n",
    "codigos = [1368, 1369, 1370, 1371, 1372, 1373, 1374, 1742, 1743, 1750, 1813, 1814, 1815]\n",
    "\n",
    "# 3. Filtrar directamente sobre el dataframe ya cargado\n",
    "df_valoracion_medica = df_valoracion_medica[df_valoracion_medica['gcodconc'].isin(codigos)].copy()\n",
    "\n",
    "# 4. Seleccionar columnas\n",
    "df_valoracion_medica = df_valoracion_medica[['gidenpac', 'gcodconc', 'ovallcon']]\n",
    "\n",
    "# 5. Guardar\n",
    "#df_valoracion_medica.to_csv('DF_VAL_MEDICAS_COMBINED.csv', index=False)\n",
    "\n",
    "# 6. Conteo\n",
    "conteo_codigos = df_valoracion_medica['ovallcon'].value_counts()\n",
    "print(conteo_codigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la nueva columna 'ds_HTA' para detectar 'HTA' en cualquier parte del texto\n",
    "df_valoracion_medica['ds_HTA'] = df_valoracion_medica['ovallcon'].str.contains('HTA', case=False, na=False).astype(int)\n",
    "df_valoracion_medica['ds_ITU'] = df_valoracion_medica['ovallcon'].str.contains('ITU', case=False, na=False).astype(int)\n",
    "df_valoracion_medica['ds_anemia'] = df_valoracion_medica['ovallcon'].str.contains('ANEMIA', case=False, na=False).astype(int)\n",
    "df_valoracion_medica['ds_vitamina_d'] = df_valoracion_medica['ovallcon'].str.contains('VITAMINA D', case=False, na=False).astype(int)\n",
    "df_valoracion_medica['ds_obesidad'] = df_valoracion_medica['ovallcon'].str.contains('OBESIDAD', case=False, na=False).astype(int)\n",
    "df_valoracion_medica['ds_osteoporosis'] = df_valoracion_medica['ovallcon'].str.contains('OSTEOPOROSIS', case=False, na=False).astype(int)\n",
    "df_valoracion_medica['ds_acido_folico'] = df_valoracion_medica['ovallcon'].str.contains('ACIDO FOLICO', case=False, na=False).astype(int)\n",
    "\n",
    "\n",
    "# Insuficiencia respiratoria\n",
    "# ___________________________________________________________\n",
    "respiratory_terms = [\n",
    "    'INSUFICIENCIA RESPIRATORIA AGUDA', 'INSUFICIENCIA RESPIRATORIA', \n",
    "    'INSUFICIENCIA RESPIRATORIA AGUDA PARCIAL', 'INSUFICIENCIA RESPIRATORIA CRÓNICA AGUDIZADA', \n",
    "    'INSUFICIENCIA RESPIRATORIA PARCIAL', 'INSUFICIENCIA RESPIRATORIA 2RIA', \n",
    "    'INSUFICIENCIA RESPIRATORIA GLOBAL', 'INSUFICIENCIA RESPIRATORIA AGUDA SECUNDARIA', \n",
    "    'INSUFICIENCIA RESPIRATORIA PARCIAL CRÓNICA AGUDIZADA', \n",
    "    'INSUFICIENCIA RESPIRATORIA AGUDA PARCIAL RESUELTA', 'EPOC AGUDIZADO', 'EPOC', 'EPOC REAGUDIZADO'\n",
    "]\n",
    "\n",
    "# Crear una expresión regular que detecte cualquiera de los términos\n",
    "pattern = '|'.join(respiratory_terms)\n",
    "\n",
    "# Crear la nueva columna 'ds_respiratoria' con 1 si encuentra cualquier término\n",
    "df_valoracion_medica['ds_insuficiencia_respiratoria'] = df_valoracion_medica['ovallcon'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "# ___________________________________________________________\n",
    "\n",
    "# Diabetes\n",
    "# ___________________________________________________________\n",
    "diabetes_terms = [\n",
    "    'DM TIPO 2 ', 'DM TIPO II', 'DIABETES MELLITUS', 'DM-II',  'DIABETES MELLITUS TIPO II','DIABETES'\n",
    "]\n",
    "\n",
    "# Crear una expresión regular que detecte cualquiera de los términos\n",
    "pattern = '|'.join(diabetes_terms)\n",
    "\n",
    "# Crear la nueva columna 'ds_respiratoria' con 1 si encuentra cualquier término\n",
    "df_valoracion_medica['ds_diabetes'] = df_valoracion_medica['ovallcon'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "# ___________________________________________________________\n",
    "\n",
    "# Infección respiratoria\n",
    "# ___________________________________________________________\n",
    "infeccion_respiratoria_terms = [\n",
    "   'INFECCIÓN RESPIRATORIA', 'INFECCIÓN RESPIRATORIA DE VÍAS BAJAS',  'INFECCIÓN RESPIRATORIA DE VÍAS INFERIORES', 'INFECCION RESPIRATORIA', 'INFECCION RESPIRATORIA AGUDA', 'INFECCIÓN RESPIRATORIA AGUDA', 'INFECCIÓN RESPIRATORIA DE VÍAS BAJAS NO CONDENSANTE'\n",
    "]\n",
    "\n",
    "# Crear una expresión regular que detecte cualquiera de los términos\n",
    "pattern = '|'.join(infeccion_respiratoria_terms)\n",
    "\n",
    "# Crear la nueva columna 'ds_respiratoria' con 1 si encuentra cualquier término\n",
    "#df_val_medicas_combined['ds_infeccion_respiratoria'] = df_val_medicas_combined['ovallcon'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "# ___________________________________________________________\n",
    "\n",
    "# Insuficiencia Cardiaca\n",
    "# ___________________________________________________________\n",
    "insuficiencia_cardiaca_terms = [\n",
    "'INSUFICIENCIA CARDIACA', 'INSUFICIENCIA CARDIACA DESCOMPENSADA',  'INSUFICIENCIA RESPIRATORIA CRÓNICA', 'INSUFICIENCIA CARDIACA CONGESTIVA',  'I. CARDIACA', 'INSUFICIENCIA CARDIACA CONGESTIVA DESCOMPENSADA', 'INSUFICIENCIA CARDÍACA', 'INSUFICIENCIA CARDIACA CRÓNICA DESCOMPENSADA',  'INSUFICIENCIA CARDIACA CONGESTIVA AGUDA', 'ICC AGUDA', 'CARDIOPATIA ISQUEMICA',  'CARDIOPATÍA ISQUÉMICA', 'ICC', 'ICC DESCOMPENSADA']\n",
    "\n",
    "# Crear una expresión regular que detecte cualquiera de los términos\n",
    "pattern = '|'.join(insuficiencia_cardiaca_terms)\n",
    "\n",
    "# Crear la nueva columna 'ds_respiratoria' con 1 si encuentra cualquier término\n",
    "df_valoracion_medica['ds_insuficiencia_cardiaca'] = df_valoracion_medica['ovallcon'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "# ___________________________________________________________\n",
    "\n",
    "# Deterioro cognitivo\n",
    "# ___________________________________________________________\n",
    "deterioro_cognitivo_terms = [\n",
    "\t'DETERIORO COGNITIVO', 'DETERIORO COGNITIVO', 'DEMENCIA']\n",
    "\n",
    "# Crear una expresión regular que detecte cualquiera de los términos\n",
    "pattern = '|'.join(deterioro_cognitivo_terms)\n",
    "\n",
    "# Crear la nueva columna 'ds_respiratoria' con 1 si encuentra cualquier término\n",
    "df_valoracion_medica['ds_deterioro_cognitivo'] = df_valoracion_medica['ovallcon'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "# ___________________________________________________________\n",
    "\n",
    "# Exitus\n",
    "# ___________________________________________________________\n",
    "exitus_terms = [ 'EXITUS', 'ÉXITUS']\n",
    "\n",
    "# Crear una expresión regular que detecte cualquiera de los términos\n",
    "pattern = '|'.join(exitus_terms)\n",
    "\n",
    "# Crear la nueva columna 'ds_respiratoria' con 1 si encuentra cualquier término\n",
    "df_valoracion_medica['ds_exitus'] = df_valoracion_medica['ovallcon'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "# ___________________________________________________________\n",
    "\n",
    "# Insuficiencia renal\n",
    "# ___________________________________________________________\n",
    "insuficiencia_renal_terms = [\n",
    "    'I. RENAL', 'INSUFICIENCIA RENAL', 'ENFERMEDAD RENAL CRÓNICA', 'INSUFICIENCIA RENAL AGUDA', 'INSUFICIENCIA RENAL CRÓNICA AGUDIZADA', 'INSUFICIENCIA RENAL CRONICA', 'IRC', 'ERC REAGUDIZADA', 'ERC AGUDIZADA', 'ERC']\n",
    "\n",
    "# Crear una expresión regular que detecte cualquiera de los términos\n",
    "pattern = '|'.join(insuficiencia_renal_terms)\n",
    "\n",
    "# Crear la nueva columna 'ds_respiratoria' con 1 si encuentra cualquier término\n",
    "df_valoracion_medica['ds_insuficiencia_renal'] = df_valoracion_medica['ovallcon'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "# ___________________________________________________________\n",
    "\n",
    "\n",
    "# Eliminamos las columnas que ya no necesitamos\n",
    "df_filtrado = df_valoracion_medica.drop(columns=['gcodconc', 'ovallcon'])\n",
    "\n",
    "# Agrupar por 'gidenpac' y aplicar max() para cada columna\n",
    "df_val_medicas_procesado = df_filtrado.groupby('gidenpac').max().reset_index()\n",
    "\n",
    "df_val_medicas_procesado.to_csv('DF_VAL_MEDICAS_PROCESED.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado de los datos finales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unir el DataFrame principal (df) con el de valoraciones medicas ya filtrado y procesado (df_val_medicas_procesado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf['ds_vitamina_d'] = df['ds_vitamina_d'].fillna(-999).astype(int)\\ndf['ds_obesidad'] = df['ds_obesidad'].fillna(-999).astype(int)\\ndf['ds_osteoporosis'] = df['ds_osteoporosis'].fillna(-999).astype(int)\\ndf['ds_insuficiencia_respiratoria'] = df['ds_insuficiencia_respiratoria'].fillna(-999).astype(int)\\ndf['ds_diabetes_y'] = df['ds_diabetes_y'].fillna(-999).astype(int)\\ndf['ds_infeccion_respiratoria'] = df['ds_infeccion_respiratoria'].fillna(-999).astype(int)\\ndf['ds_insuficiencia_cardiaca'] = df['ds_insuficiencia_cardiaca'].fillna(-999).astype(int)\\ndf['ds_deterioro_cognitivo'] = df['ds_deterioro_cognitivo'].fillna(-999).astype(int)\\ndf['ds_insuficiencia_renal'] = df['ds_insuficiencia_renal'].fillna(-999).astype(int)\\ndf['ds_HTA_y'] = df['ds_HTA_y'].fillna(-999).astype(int)\\ndf['ds_ITU'] = df['ds_ITU'].fillna(-999).astype(int)\\ndf['ds_acido_folico'] = df['ds_acido_folico'].fillna(-999).astype(int)\\ndf['ds_anemia'] = df['ds_anemia'].fillna(-999).astype(int)\\n\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unir df y df_val_medicas_procesado por la columna 'gidenpac'\n",
    "df = df.merge(df_val_medicas_procesado, on='gidenpac', how='left')\n",
    "# Reemplazar valores NaN o nulos por -999\n",
    "#df.fillna(-999, inplace=True)\n",
    "\n",
    "# Los valores vacios los imputamos al valor común, en mas de un 99% de los casos que es 0.0, sin enfermedad. Otra estrategia seria eliminarlos, pero de momento al tener pocos datos, lo dejamos así.\n",
    "# Lista de las columnas de diagnósticos mostradas en la imagen\n",
    "cols_diagnosticos = [\n",
    "    'ds_ITU', \n",
    "    'ds_anemia', \n",
    "    'ds_vitamina_d', \n",
    "    'ds_insuficiencia_respiratoria',  # Asumo el nombre completo\n",
    "    'ds_insuficiencia_cardiaca', \n",
    "    'ds_deterioro_cognitivo', \n",
    "    'ds_insuficiencia_renal',   \n",
    "    'ds_obesidad',\n",
    "    'ds_osteoporosis',\n",
    "    'ds_acido_folico'\n",
    "]\n",
    "\n",
    "# Verificar que las columnas existan antes de intentar rellenarlas (para evitar errores)\n",
    "cols_existentes = [col for col in cols_diagnosticos if col in df.columns]\n",
    "\n",
    "# Rellenar los valores vacíos (NaN) con 0.0 en esas columnas\n",
    "df[cols_existentes] = df[cols_existentes].fillna(0.0)\n",
    "\n",
    "# Convertimos a enteros y los Nan los pasamos a -999\n",
    "\"\"\"\n",
    "df['ds_vitamina_d'] = df['ds_vitamina_d'].fillna(-999).astype(int)\n",
    "df['ds_obesidad'] = df['ds_obesidad'].fillna(-999).astype(int)\n",
    "df['ds_osteoporosis'] = df['ds_osteoporosis'].fillna(-999).astype(int)\n",
    "df['ds_insuficiencia_respiratoria'] = df['ds_insuficiencia_respiratoria'].fillna(-999).astype(int)\n",
    "df['ds_diabetes_y'] = df['ds_diabetes_y'].fillna(-999).astype(int)\n",
    "df['ds_infeccion_respiratoria'] = df['ds_infeccion_respiratoria'].fillna(-999).astype(int)\n",
    "df['ds_insuficiencia_cardiaca'] = df['ds_insuficiencia_cardiaca'].fillna(-999).astype(int)\n",
    "df['ds_deterioro_cognitivo'] = df['ds_deterioro_cognitivo'].fillna(-999).astype(int)\n",
    "df['ds_insuficiencia_renal'] = df['ds_insuficiencia_renal'].fillna(-999).astype(int)\n",
    "df['ds_HTA_y'] = df['ds_HTA_y'].fillna(-999).astype(int)\n",
    "df['ds_ITU'] = df['ds_ITU'].fillna(-999).astype(int)\n",
    "df['ds_acido_folico'] = df['ds_acido_folico'].fillna(-999).astype(int)\n",
    "df['ds_anemia'] = df['ds_anemia'].fillna(-999).astype(int)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a combinar las 2 columnas de HTA.\n",
    "- HTA: Tenemos 2. X e Y. \n",
    "\t- X viene de antecedentes\n",
    "\t- Y de valoraciones médicas\n",
    "- Podemos combinar las 2, ya que es más probable que no pongan la info, a poner una info falsa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ds_HTA'] = df[['ds_HTA_x', 'ds_HTA_y']].max(axis=1)\n",
    "\n",
    "# Eliminar las columnas originales\n",
    "df.drop(columns=['ds_HTA_x', 'ds_HTA_y'], inplace=True)\n",
    "\n",
    "# Igual con diabetes\n",
    "df['ds_diabetes'] = df[['ds_diabetes_x', 'ds_diabetes_y']].max(axis=1)\n",
    "\n",
    "# Eliminar las columnas originales\n",
    "df.drop(columns=['ds_diabetes_x', 'ds_diabetes_y'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos filas que les falten datos importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Si las valoraciones médicas no tienen info sobre el paciente, lo eliminamos\\ncolumnas_a_verificar = [\\n    'ds_ITU', 'ds_acido_folico', 'ds_anemia', 'ds_vitamina_d', \\n    'ds_obesidad', 'ds_osteoporosis', 'ds_insuficiencia_respiratoria', \\n    'ds_infeccion_respiratoria', 'ds_insuficiencia_cardiaca', \\n    'ds_deterioro_cognitivo', 'ds_insuficiencia_renal'\\n]\\n\\ndf = df[~df[columnas_a_verificar].isin([-999]).any(axis=1)]\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminamos filas que no tengan los dias de pre o post operacion\n",
    "df = df[df['ds_pre_oper'] != -999]  # Eliminamos filas que no tengan los dias de pre o post operacion\n",
    "df = df[df['ds_post_oper'] != -999]\n",
    "\"\"\"\n",
    "# Si las valoraciones médicas no tienen info sobre el paciente, lo eliminamos\n",
    "columnas_a_verificar = [\n",
    "    'ds_ITU', 'ds_acido_folico', 'ds_anemia', 'ds_vitamina_d', \n",
    "    'ds_obesidad', 'ds_osteoporosis', 'ds_insuficiencia_respiratoria', \n",
    "    'ds_infeccion_respiratoria', 'ds_insuficiencia_cardiaca', \n",
    "    'ds_deterioro_cognitivo', 'ds_insuficiencia_renal'\n",
    "]\n",
    "\n",
    "df = df[~df[columnas_a_verificar].isin([-999]).any(axis=1)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prescripción Farmacos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medicamentos prescitos\n",
    "\n",
    "# Seleccionar solo las columnas 'gidenpac' y 'tarasist'\n",
    "df_prescripcion_farmacos = df_prescripcion_farmacos[['gidenpac', 'tarasist']]\n",
    "\n",
    "# Agrupar por 'gidenpac' y combinar los medicamentos en una lista separada por comas\n",
    "df_medicamentos = df_prescripcion_farmacos.groupby('gidenpac')['tarasist'].apply(lambda x: ', '.join(x.dropna().astype(str))).reset_index()\n",
    "\n",
    "# Guardar\n",
    "df_medicamentos.to_csv('medicamentos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesado final para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a categorico\n",
    "columnas_a_convertir = ['itipsexo','ireingre','gsitalta', 'gmotalta','gdiagalt','ds_izq_der','ds_turno','ds_vivo_alta','ds_dia_semana_llegada', 'ds_mes_llegada', 'ds_HTA','ds_alergia_medicamentosa','ds_alergia_alimenticia','ds_diabetes','ds_otras_alergias', 'lugar_residencia','lugar_procedencia','destino_alta','ds_ITU','ds_anemia','ds_vitamina_d','ds_insuficiencia_respiratoria','ds_insuficiencia_cardiaca','ds_deterioro_cognitivo','ds_insuficiencia_renal' ]\n",
    "df[columnas_a_convertir] = df[columnas_a_convertir].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"gdiagalt\",\"ds_izq_der\",\"ds_dia_semana_llegada\", \"ds_mes_llegada\", \"ds_turno\"]\n",
    "\n",
    "cols_onehot = [c for c in cols\n",
    "               if c in df.columns]\n",
    "\n",
    "df = pd.get_dummies(df, columns=cols_onehot, drop_first=False, dtype=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar columnas que no tienen informacion suficiente para utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la lista de columnas a eliminar o no necesitamos\n",
    "cols_to_drop = ['gidenpac','ds_exitus', 'gcodipos', 'itipingr', 'ireingre', 'gmotalta', 'ds_vivo_alta','lugar_residencia','lugar_procedencia','destino_alta','ds_obesidad','ds_acido_folico',\n",
    "                'faltplan','fnacipac','fllegada','finterve'\n",
    "]\n",
    "\n",
    "df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Eliminamos columnas de gdiagalt que no aportan info porque solo tienen 1 registro\n",
    "'''gdiagalt_to_drop = ['G45.9', 'G72.81', 'I69.152', 'I69.254', 'M51.26', 'M80.051A', 'M97.01XA', 'M97.02XA', 'S72.111A','S72.142D','S72.21XA','S72.24XA','S73.031A', 'T84.010A', \n",
    "                    'T84.011A','T84.030A', 'T84.051A', 'Z47.81'\n",
    "]'''\n",
    "gdiagalt_to_drop = []\n",
    "\n",
    "# Añadimos el prefijo 'gdiagalt_' a cada elemento\n",
    "full_cols_to_drop = ['gdiagalt_' + code for code in gdiagalt_to_drop]\n",
    "\n",
    "# Ahora sí borramos las columnas completas\n",
    "df.drop(columns=full_cols_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupamos categorias de gsitalta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1: Curación total\\n2: Mejoria\\n3: Sin cambios\\n4: Agravamiento\\n6: Con secuelas\\n7: Exitus\\n9: Mejoria a residencia\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def agrupar_situacion_alta(valor):\n",
    "    if valor in [1, 2, 9]:\n",
    "        return 2\n",
    "    elif valor in [3, 4, 6,7]:\n",
    "        return 4\n",
    "    else:\n",
    "        return 99 # Categoría de seguridad por si escapa algún valor\n",
    "\n",
    "df['gsitalta_agrupada'] = df['gsitalta'].apply(agrupar_situacion_alta)\n",
    "\n",
    "'''\n",
    "1: Curación total\n",
    "2: Mejoria\n",
    "3: Sin cambios\n",
    "4: Agravamiento\n",
    "6: Con secuelas\n",
    "7: Exitus\n",
    "9: Mejoria a residencia\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos las columnas numéricas a su tipo adecuado\n",
    "# Lista de columnas a convertir\n",
    "columnas_categoricas = ['itipsexo', 'gsitalta', 'gsitalta_agrupada','iotrocen','ds_centro_afueras',]\n",
    "\n",
    "# Bucle para convertir cada columna\n",
    "for col in columnas_categoricas:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalar los valores numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 1. Escalar X (Predictoras)\\ncols_numericas = ['ds_edad','ds_estancia', 'ds_pre_oper','ds_post_oper', 'ntensmin', 'ntensmax', 'ntempera', 'nsatuoxi', 'Barthel', 'braden']\\n\\nscaler = StandardScaler()\\n# Sobrescribimos en X los valores transformados\\ndf[cols_numericas] = scaler.fit_transform(df[cols_numericas])\\n\\n# 4. Guardar el Scaler (necesario para desescalar resultados futuros)\\njoblib.dump(scaler, 'scaler_completo.pkl') #(Para preprocesar nuevos pacientes cuando el modelo de resultados, ya que si se los damos escalados, el los devuelve escalados y debemos desescalar para ver los días)\\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- ESCALADO Y GUARDADO --- Mejor hacerlo en el pipeline de entrenamiento\n",
    "'''\n",
    "# 1. Escalar X (Predictoras)\n",
    "cols_numericas = ['ds_edad','ds_estancia', 'ds_pre_oper','ds_post_oper', 'ntensmin', 'ntensmax', 'ntempera', 'nsatuoxi', 'Barthel', 'braden']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Sobrescribimos en X los valores transformados\n",
    "df[cols_numericas] = scaler.fit_transform(df[cols_numericas])\n",
    "\n",
    "# 4. Guardar el Scaler (necesario para desescalar resultados futuros)\n",
    "joblib.dump(scaler, 'scaler_completo.pkl') #(Para preprocesar nuevos pacientes cuando el modelo de resultados, ya que si se los damos escalados, el los devuelve escalados y debemos desescalar para ver los días)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Cargar el scaler\\nscaler = joblib.load('scaler_completo.pkl')\\n\\n# Índice de 'ds_estancia' en tu lista original (es el índice 1)\\nidx_estancia = 1 \\nmedia = scaler.mean_[idx_estancia]\\ndesviacion = scaler.scale_[idx_estancia]\\n\\n# Desescalar manual (Valor Real = (Valor Escalado * Desviación) + Media)\\nprediccion_dias_reales = (y_pred * desviacion) + media\\n\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "'''\n",
    "# Cargar el scaler\n",
    "scaler = joblib.load('scaler_completo.pkl')\n",
    "\n",
    "# Índice de 'ds_estancia' en tu lista original (es el índice 1)\n",
    "idx_estancia = 1 \n",
    "media = scaler.mean_[idx_estancia]\n",
    "desviacion = scaler.scale_[idx_estancia]\n",
    "\n",
    "# Desescalar manual (Valor Real = (Valor Escalado * Desviación) + Media)\n",
    "prediccion_dias_reales = (y_pred * desviacion) + media\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('DATOS_PREPROCESADOS.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
